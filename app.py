import os
from typing import List
import chromadb
from sentence_transformers import SentenceTransformer
from google import genai

# === Load Text File ===
def load_document(file_path: str) -> str:
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

# === Split Text into Chunks ===
def split_into_chunks(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunk = text[i:i + chunk_size]
        chunks.append(chunk.strip())
    return chunks

# === Initialize ChromaDB and Embedder ===
chroma_client = chromadb.PersistentClient(path="./chroma_store")  # new syntax
collection = chroma_client.get_or_create_collection(name="my_txt_collection")
embedder = SentenceTransformer("all-MiniLM-L6-v2")  # Open-source and fast

# === Embed and Store Chunks ===
def embed_and_store(chunks: List[str]):
    embeddings = embedder.encode(chunks).tolist()
    ids = [f"chunk_{i}" for i in range(len(chunks))]
    collection.add(documents=chunks, embeddings=embeddings, ids=ids)

# === Query with a Question ===
def query_document(question: str, top_k: int = 200) -> List[str]:
    query_embedding = embedder.encode([question]).tolist()[0]
    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)
    return results["documents"]

# === Use Gemini Model for Answer Generation ===
def generate_answer_from_gemini(chunks: List[str], question: str) -> str:
    # Flatten the list of chunks (if it's a list of lists, otherwise proceed)
    flat_chunks = [chunk for sublist in chunks for chunk in (sublist if isinstance(sublist, list) else [sublist])]

    # Combine top_k chunks into a prompt
    prompt = f"Question: {question}\n\nContext:\n" + "\n".join(flat_chunks) + "\n\nAnswer:"
    
    # Initialize the Google Gemini Client with your API Key
    client = genai.Client(api_key="AIzaSyB9fD0L8l-y_C-_bA8auAmY6phc03n5sIg")
    
    # Generate the content using the Google Gemini Model
    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=[prompt]
    )
    return response.text

# === Main Pipeline ===
if __name__ == "__main__":
    file_path = "debug.log"  # replace with your .txt file

    if not os.path.exists(file_path):
        print(f"‚ùå Error: File not found at {file_path}")
        exit(1)

    # Load, split, embed, store
    print("üìÑ Loading document...")
    text = load_document(file_path)

    print("‚úÇÔ∏è Splitting into chunks...")
    chunks = split_into_chunks(text)

    print("üì¶ Embedding and storing in ChromaDB...")
    embed_and_store(chunks)

    # Ask questions interactively
    print("\n‚úÖ Document is ready for question answering!")
    while True:
        question = input("\n‚ùì Ask a question (or type 'exit'): ")
        if question.lower() in ["exit", "quit"]:
            break
        
        # Step 1: Query ChromaDB for top_k=200 relevant chunks
        top_k_chunks = query_document(question, top_k=908)
        
        # Step 2: Generate the final answer using Google Gemini
        answer = generate_answer_from_gemini(top_k_chunks, question)
        
        # Print the answer
        print("\nüí° Answer Generated by Gemini:\n")
        print(answer)
